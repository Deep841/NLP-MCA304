{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53cd226",
   "metadata": {},
   "source": [
    "## üìù NLP Pipeline - Step 2: Text Preprocessing\n",
    "\n",
    "### 1Ô∏è‚É£ Cleaning\n",
    "In this step, we remove unwanted parts from text to make it ready for further processing.  \n",
    "We will cover:\n",
    "- HTML tag removal\n",
    "- Unicode normalization\n",
    "- Emoji conversion\n",
    "- Spelling correction\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Basic Preprocessing\n",
    "2 types h isme (I)Basic (II)Optional \n",
    "- (I)Basic : Tokenization ... jisme : \n",
    "- - senetence tokenization \n",
    "- - word tokenization\n",
    "\n",
    "- (II)Optional :\n",
    "- - stop word removal\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Advanced Preprocessing\n",
    "(To be covered later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e59eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: <p>This is <b>bold</b> and <i>italic</i> text.</p>\n",
      "After: This is bold and italic text.\n"
     ]
    }
   ],
   "source": [
    "#this is example of data cleaning : removing html tags \n",
    "import re\n",
    "\n",
    "# Sample text with HTML tags\n",
    "sample_text = \"<p>This is <b>bold</b> and <i>italic</i> text.</p>\"\n",
    "\n",
    "# Function to strip HTML tags using regex\n",
    "def strip_html(data):\n",
    "    pattern = re.compile(r'<.*?>')  # matches anything between < and >\n",
    "    return pattern.sub('', data)    # replace tags with empty string\n",
    "\n",
    "# Apply function\n",
    "clean_text = strip_html(sample_text)\n",
    "\n",
    "print(\"Before:\", sample_text)\n",
    "print(\"After:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18ff9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (2.14.1)\n",
      "\n",
      "Original text: I am happy üòÑ and I love pizza üçï!\n",
      "Normalized text: I am happy üòÑ and I love pizza üçï!\n",
      "Emoji converted text: I am happy  grinning face with smiling eyes  and I love pizza  pizza !\n"
     ]
    }
   ],
   "source": [
    "#this is an example of data cleaning : unicode normalization - emojis to machine understandable text\n",
    "# Example: Unicode Normalization - Emojis to Machine Understandable Text\n",
    "!pip install emoji\n",
    "print()\n",
    "import unicodedata\n",
    "import emoji\n",
    "\n",
    "# Sample text containing emojis\n",
    "sample_text = \"I am happy üòÑ and I love pizza üçï!\"\n",
    "\n",
    "# Step 1: Unicode Normalization (NFC form)\n",
    "normalized_text = unicodedata.normalize(\"NFC\", sample_text)\n",
    "\n",
    "# Step 2: Convert emoji to text descriptions\n",
    "emoji_converted_text = emoji.demojize(normalized_text, language='en')\n",
    "\n",
    "# Step 3: Make it more ML-friendly (replace underscores with spaces)\n",
    "emoji_converted_text = emoji_converted_text.replace(\"_\", \" \").replace(\":\", \" \")\n",
    "\n",
    "print(\"Original text:\", sample_text)\n",
    "print(\"Normalized text:\", normalized_text)\n",
    "print(\"Emoji converted text:\", emoji_converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b0c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "#lib(s) for spell checking \n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd718e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I havv goood speling.\n",
      "Corrected Text: I have good spelling.\n"
     ]
    }
   ],
   "source": [
    "#this is \n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text with spelling mistakes\n",
    "text = \"I havv goood speling.\"\n",
    "\n",
    "# Create TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Correct the spelling\n",
    "corrected_text = blob.correct()\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Corrected Text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe851d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /Users/deeplatiyan/miniconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53abdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      "['I love Python.', 'It is great for NLP!']\n",
      "Word Tokenization:\n",
      "['I', 'love', 'Python', '.', 'It', 'is', 'great', 'for', 'NLP', '!']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Basic Pre processing : \n",
    "#Tokenization : sentence tokenization & word tokenization\n",
    "\n",
    "# Step 2: Import required libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download tokenizer models (Run only once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"I love Python. It is great for NLP!\"\n",
    "\n",
    "# 1Ô∏è‚É£ Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\")\n",
    "print(sentences)\n",
    "\n",
    "# 2Ô∏è‚É£ Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization:\")\n",
    "print(words)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc33d2",
   "metadata": {},
   "source": [
    "## üìå Stop Words Removal Example\n",
    "Stop words are common words (e.g., is, the, and) that are removed to keep only meaningful words for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6296e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a sample sentence, showing off the stop words filtration.\n",
      "After stop words removal: ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "# Tokenize words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"After stop words removal:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4cd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
