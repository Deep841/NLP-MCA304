{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f41726d",
   "metadata": {},
   "source": [
    "### CountVectorizer :\n",
    "- it is a tool provided by Scikit-learn(skkearn) in python that converts a collection of text documents into a matrix of token counts.\n",
    "- simply, it converts text data into numerical form \n",
    "\n",
    "---\n",
    "\n",
    "### why use? :\n",
    "- ML models cannot understand text directly - they need numbers. CountVectorizer helps in this by :\n",
    "- breaking down text into words (called tokens)\n",
    "- countinhg how many times each word appears\n",
    "- representing the text as a numerical vector\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step Working of CountVecotrizer :\n",
    "corpus_old = [\"i love NLP\", \"NLP loves me\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a43db7",
   "metadata": {},
   "source": [
    "1. - Tokenization : breaks each sentence into words\n",
    "- [\"I\", \"love\", \"NLP\", \"loves\", \"me\"]\n",
    "\n",
    "2. - Build vocabulary : list of unique words \n",
    "- [\"i\", \"love\", \"nlp\", \"loves\", \"me\"]\n",
    "\n",
    "3. - Count word frquencies : for each sentence\n",
    "- Sentence 1: [1,1,1,0,0]\n",
    "- Sentence 2: [0,0,1,1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86185d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['love' 'loves' 'me' 'nlp']\n",
      "BoW Matrix:\n",
      " [[1 0 0 1]\n",
      " [0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\"I love NLP\", \"NLP loves me\"]\n",
    "\n",
    "# Create CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print feature names\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the matrix\n",
    "print(\"BoW Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "299bafdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'language' 'love' 'natural' 'nlp' 'processing']\n",
      "[[0 0 1 1 1 0 1]\n",
      " [1 1 1 0 0 0 1]\n",
      " [0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Language processing is fun.\",\n",
    "    \"I love NLP!\"\n",
    "]\n",
    "\n",
    "\n",
    "# 1. Basic use :\n",
    "v = CountVectorizer()\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26093928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'language' 'love' 'natural' 'nlp' 'processing']\n",
      "['Language' 'NLP' 'fun' 'is' 'language' 'love' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# 2. Using lowercase \n",
    "v = CountVectorizer(lowercase=True)\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())\n",
    "\n",
    "v2 = CountVectorizer(lowercase=False)\n",
    "X2 = v2.fit_transform(corpus)\n",
    "print(v2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "813044c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'language' 'love' 'natural' 'nlp' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# 3. Stop Words \n",
    "\n",
    "v = CountVectorizer(stop_words = 'english')\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5beda99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'is fun' 'language' 'language processing' 'love'\n",
      " 'love natural' 'love nlp' 'natural' 'natural language' 'nlp' 'processing'\n",
      " 'processing is']\n"
     ]
    }
   ],
   "source": [
    "# 4. ngram\n",
    "#ngram_range=(1,2) for using unigrams and bigrams\n",
    "\n",
    "v = CountVectorizer(ngram_range=(1,2))\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9ed2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' f' ' fu' ' fun' ' i' ' is' ' is ' ' l' ' la' ' lan' ' lo' ' lov' ' n'\n",
      " ' na' ' nat' ' nl' ' nlp' ' p' ' pr' ' pro' 'ag' 'age' 'age ' 'al' 'al '\n",
      " 'al l' 'an' 'ang' 'angu' 'at' 'atu' 'atur' 'ce' 'ces' 'cess' 'e ' 'e n'\n",
      " 'e na' 'e nl' 'e p' 'e pr' 'es' 'ess' 'essi' 'fu' 'fun' 'fun.' 'g ' 'g i'\n",
      " 'g is' 'g.' 'ge' 'ge ' 'ge p' 'gu' 'gua' 'guag' 'i ' 'i l' 'i lo' 'in'\n",
      " 'ing' 'ing ' 'ing.' 'is' 'is ' 'is f' 'l ' 'l l' 'l la' 'la' 'lan' 'lang'\n",
      " 'lo' 'lov' 'love' 'lp' 'lp!' 'n.' 'na' 'nat' 'natu' 'ng' 'ng ' 'ng i'\n",
      " 'ng.' 'ngu' 'ngua' 'nl' 'nlp' 'nlp!' 'oc' 'oce' 'oces' 'ov' 'ove' 'ove '\n",
      " 'p!' 'pr' 'pro' 'proc' 'ra' 'ral' 'ral ' 'ro' 'roc' 'roce' 's ' 's f'\n",
      " 's fu' 'si' 'sin' 'sing' 'ss' 'ssi' 'ssin' 'tu' 'tur' 'tura' 'ua' 'uag'\n",
      " 'uage' 'un' 'un.' 'ur' 'ura' 'ural' 've' 've ' 've n']\n"
     ]
    }
   ],
   "source": [
    "# 5. analyzer = 'char\n",
    "# character level analysis \n",
    "v = CountVectorizer(analyzer='char', ngram_range=(2,4))\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28b5df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 1]\n",
      " [1 1 1 0 0 0 1]\n",
      " [0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 6. binary bag-of-words\n",
    "\n",
    "v = CountVectorizer(binary=True)\n",
    "X = v.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0122a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'language' 'love' 'natural' 'nlp' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# 7. max_df & min_df\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0aaf36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 8. Using a custom vocabulary\n",
    "v = CountVectorizer(vocabulary=['language', 'nlp', 'love'])\n",
    "X = v.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be9b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'i' 'is' 'language' 'love' 'natural' 'nlp' 'processing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deeplatiyan/miniconda3/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 9. Custom Tokenizer :\n",
    "\n",
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "v = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acdae58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Language' 'fun' 'is' 'language' 'love' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# 10. custom preprocessor : \n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text.replace(\"NLP\", \"natural language processing\")\n",
    "\n",
    "v = CountVectorizer(preprocessor=custom_preprocessor)\n",
    "X = v.fit_transform(corpus)\n",
    "print(v.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db622f4a",
   "metadata": {},
   "source": [
    "#### Summary \n",
    "I explored all the major parameters of CountVectorizer, including stop_words, ngram_range, analyzer, binary, min_df, max_df, custom tokenizer, and preprocessor. Each parameter affects how the text is tokenized and represented as a vector. The flexibility of CountVectorizer makes it powerful for different kinds of NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
