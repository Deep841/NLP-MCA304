{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57043488",
   "metadata": {},
   "source": [
    "## Assignment - 1:\n",
    "### Sentiment Anaysis on IMBD Dataset with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b0e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      1\n",
       "1  A wonderful little production. <br /><br />The...  positive      1\n",
       "2  I thought this was a wonderful way to spend ti...  positive      1\n",
       "3  Basically there's a family where a little boy ...  negative      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#loading CSV file\n",
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "\n",
    "#adding numeric label (0=negative, 1=positive)\n",
    "df['label'] = df['sentiment'].map({'negative' : 0, 'positive':1})\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "#sentiment is converted to number (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09eff3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train - Test Split\n",
    "# splitting into training and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "print(X_train.shape, X_val.shape)\n",
    "\n",
    "# stratify = y ensures equal distribution of positive/negative in both train & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i didn t like this movie br visit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deeplatiyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Text CLeaning\n",
    "#converting reviews into cleaner text\n",
    "\n",
    "# Theory : LowerCasing reduces vocab size.\n",
    "#        : Remove HTML tags, URLS, punctaitons, digits (noise).\n",
    "#        : Keep negations like \"not\" as they change sentiment.\n",
    "\n",
    "import re, html\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords') #comented after downloading.\n",
    "\n",
    "#to Keep negations\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for neg in ['not', 'no', 'nor', 'never']:\n",
    "    stop_words.discard(neg)\n",
    "    \n",
    "def clean_text(text):\n",
    "    #removing html tags :\n",
    "    text = re.sub('r<[^>]+>', ' ', text)\n",
    "    \n",
    "    #decoding HTML entities \n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    #removing URLS :\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \" \", text)\n",
    "    \n",
    "    #lowercasing : \n",
    "    text = text.lower()\n",
    "    \n",
    "    #removing punctuatuins/digits :\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    \n",
    "    #removing extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \",text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "print(clean_text(\"I didn't LIKE this movie! <br> Visit: http://abc.com\"))\n",
    "\n",
    "#if we remove \"not\", nodek may misclassify \"not good\" as \"good\"\n",
    "#normalised spacing after regex replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40eee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Extraction(Bag-of-Words & TF-IDF)\n",
    "# convering text to numbers using COuntVectorizer(BOW) and TfidfVectorizer.\n",
    "\n",
    "#Theory : \n",
    "#   Bag-of-Words : counts word frequency. works wll with MultinomialNB\n",
    "#   TF-IDF : scales down very common words, boosts rare but useful words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(preprocessor=clean_text,\n",
    "                                 stop_words=stop_words,\n",
    "                                 ngram_range=(1,2),   # unigrams + bigrams\n",
    "                                 min_df=5, max_df=0.9)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=clean_text,\n",
    "                                   stop_words=stop_words,\n",
    "                                   ngram_range=(1,2),\n",
    "                                   min_df=5, max_df=0.9)\n",
    "\n",
    "#adding both bigrams (1,2) captures phrases like \"not good\".\n",
    "#min_df = 5 ignores very rare words.\n",
    "#max_df = 0.9 ignores extremnly frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0c5ed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'just', 'with', 'this', 'which', 'against', 'mightn', 'ourselves', 'has', \"it'd\", 'mustn', 'its', \"we'd\", 'does', 'any', 'why', \"you've\", 'from', 'couldn', 'such', 'at', \"mustn't\", \"couldn't\", 'yourselves', 'those', 're', 'don', \"haven't\", 'of', 'should', 'our', 'them', 'you', 'didn', 'during', 'shouldn', 'into', \"they'd\", 'we', 'myself', 'wouldn', 'while', 'having', 'was', \"he'll\", 'm', 'being', 'weren', 'will', 'so', 'an', 'for', 'where', 'about', 'few', \"she's\", 'some', 'because', 'but', 'her', \"i'll\", 'or', \"he'd\", 'ain', 'doing', 'other', \"hadn't\", 'needn', 'shan', 'me', 'up', 'it', 'had', 'be', \"that'll\", 'between', 'i', 'before', \"we've\", 's', \"they'll\", 'to', 'now', 've', 'o', 'whom', 'who', 'been', \"hasn't\", 'his', 'd', 'all', 'above', 'my', 'are', \"weren't\", 'by', 'do', 'most', 'only', \"wasn't\", \"aren't\", 'they', 'themselves', 'ma', \"i've\", 'down', 'yourself', 'hasn', 't', 'over', 'wasn', \"it'll\", 'hers', 'again', \"she'll\", 'doesn', 'once', 'after', 'what', 'both', 'here', \"they're\", 'that', 'ours', 'yours', 'won', 'did', 'out', 'aren', 'through', \"he's\", 'isn', 'the', \"we'll\", \"shouldn't\", \"isn't\", 'as', \"it's\", \"i'm\", 'll', \"shan't\", 'haven', 'their', 'theirs', 'very', 'and', 'himself', 'than', \"don't\", \"doesn't\", 'he', \"i'd\", \"they've\", 'she', \"you're\", 'under', 'on', 'y', \"you'd\", 'there', 'each', 'is', 'itself', 'a', \"didn't\", \"mightn't\", 'these', 'can', 'more', 'same', 'too', \"should've\", \"won't\", 'off', 'your', 'in', \"you'll\", 'have', 'him', 'herself', 'below', 'until', \"we're\", 'if', 'further', 'am', 'how', 'were', \"needn't\", 'when', 'hadn', 'own', 'then', \"wouldn't\", \"she'd\"} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m *\u001b[32m80\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMultinomialNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m train_and_eval(tfidf_vectorizer, MultinomialNB(), X_train, X_val, y_train, y_val)\n\u001b[32m     33\u001b[39m train_and_eval(bow_vectorizer, BernoulliNB(), X_train, X_val, y_train, y_val)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_and_eval\u001b[39m\u001b[34m(vec, clg, X_train, X_val, y_train, y_val)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_eval\u001b[39m(vec , clg, X_train, X_val, y_train, y_val):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m#vectorize : \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     X_train_vec = \u001b[43mvec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     X_val_vec = vec.tranform(X_val)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m#fit :\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/base.py:1358\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1353\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1354\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1355\u001b[39m )\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m   1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/base.py:471\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    465\u001b[39m \n\u001b[32m    466\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'just', 'with', 'this', 'which', 'against', 'mightn', 'ourselves', 'has', \"it'd\", 'mustn', 'its', \"we'd\", 'does', 'any', 'why', \"you've\", 'from', 'couldn', 'such', 'at', \"mustn't\", \"couldn't\", 'yourselves', 'those', 're', 'don', \"haven't\", 'of', 'should', 'our', 'them', 'you', 'didn', 'during', 'shouldn', 'into', \"they'd\", 'we', 'myself', 'wouldn', 'while', 'having', 'was', \"he'll\", 'm', 'being', 'weren', 'will', 'so', 'an', 'for', 'where', 'about', 'few', \"she's\", 'some', 'because', 'but', 'her', \"i'll\", 'or', \"he'd\", 'ain', 'doing', 'other', \"hadn't\", 'needn', 'shan', 'me', 'up', 'it', 'had', 'be', \"that'll\", 'between', 'i', 'before', \"we've\", 's', \"they'll\", 'to', 'now', 've', 'o', 'whom', 'who', 'been', \"hasn't\", 'his', 'd', 'all', 'above', 'my', 'are', \"weren't\", 'by', 'do', 'most', 'only', \"wasn't\", \"aren't\", 'they', 'themselves', 'ma', \"i've\", 'down', 'yourself', 'hasn', 't', 'over', 'wasn', \"it'll\", 'hers', 'again', \"she'll\", 'doesn', 'once', 'after', 'what', 'both', 'here', \"they're\", 'that', 'ours', 'yours', 'won', 'did', 'out', 'aren', 'through', \"he's\", 'isn', 'the', \"we'll\", \"shouldn't\", \"isn't\", 'as', \"it's\", \"i'm\", 'll', \"shan't\", 'haven', 'their', 'theirs', 'very', 'and', 'himself', 'than', \"don't\", \"doesn't\", 'he', \"i'd\", \"they've\", 'she', \"you're\", 'under', 'on', 'y', \"you'd\", 'there', 'each', 'is', 'itself', 'a', \"didn't\", \"mightn't\", 'these', 'can', 'more', 'same', 'too', \"should've\", \"won't\", 'off', 'your', 'in', \"you'll\", 'have', 'him', 'herself', 'below', 'until', \"we're\", 'if', 'further', 'am', 'how', 'were', \"needn't\", 'when', 'hadn', 'own', 'then', \"wouldn't\", \"she'd\"} instead."
     ]
    }
   ],
   "source": [
    "# Step 5: Train Models (MultinomialNB, BernoulliNB)\n",
    "# lets train n compare both NB classifiers \n",
    "\n",
    "#Theory : \n",
    "#MultinomialNB : Best for words counts or tf-idf(works on frequencies).\n",
    "#BernoulliNB : Best when features are binary (word present / absent).\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_and_eval(vec , clg, X_train, X_val, y_train, y_val):\n",
    "    #vectorize : \n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_val_vec = vec.tranform(X_val)\n",
    "    \n",
    "    #fit :\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    \n",
    "    #predict :\n",
    "    preds = clf.predict(X_val_vec)\n",
    "    \n",
    "    #evaluare :\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    print(f\"Model: {clf.__class__.__name__}, Vectorizer: {vec.__class__.__name__}, Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_val, preds, digits=4))\n",
    "    print(\"=\" *80)\n",
    "    \n",
    "\n",
    "# Run experiments\n",
    "train_and_eval(bow_vectorizer, MultinomialNB(), X_train, X_val, y_train, y_val)\n",
    "train_and_eval(tfidf_vectorizer, MultinomialNB(), X_train, X_val, y_train, y_val)\n",
    "\n",
    "train_and_eval(bow_vectorizer, BernoulliNB(), X_train, X_val, y_train, y_val)\n",
    "train_and_eval(tfidf_vectorizer, BernoulliNB(), X_train, X_val, y_train, y_val)\n",
    "\n",
    "#accuracy n precision/recall are printed\n",
    "#normally, MultinomialNB with TF-IDF gives best performance (!85-90%)\n",
    "#BernoulliNB can be weaker unless features are stricly binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0be661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')   # list, not set\n",
    "for neg in ['not','no','nor','never']:\n",
    "    if neg in stop_words:\n",
    "        stop_words.remove(neg)   # keep negations\n",
    "\n",
    "bow_vectorizer = CountVectorizer(preprocessor=clean_text,\n",
    "                                 stop_words=stop_words,\n",
    "                                 ngram_range=(1,2),   \n",
    "                                 min_df=5, max_df=0.9)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=clean_text,\n",
    "                                   stop_words=stop_words,\n",
    "                                   ngram_range=(1,2),\n",
    "                                   min_df=5, max_df=0.9)\n",
    "\n",
    "#to fix above error\n",
    "#converted stopwords to list and\n",
    "#fixed tying error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ec75b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultinomialNB, Vectorizer: CountVectorizer, Accuracy: 0.8849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8825    0.8880    0.8853      5000\n",
      "           1     0.8873    0.8818    0.8845      5000\n",
      "\n",
      "    accuracy                         0.8849     10000\n",
      "   macro avg     0.8849    0.8849    0.8849     10000\n",
      "weighted avg     0.8849    0.8849    0.8849     10000\n",
      "\n",
      "================================================================================\n",
      "Model: MultinomialNB, Vectorizer: TfidfVectorizer, Accuracy: 0.8919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8942    0.8890    0.8916      5000\n",
      "           1     0.8896    0.8948    0.8922      5000\n",
      "\n",
      "    accuracy                         0.8919     10000\n",
      "   macro avg     0.8919    0.8919    0.8919     10000\n",
      "weighted avg     0.8919    0.8919    0.8919     10000\n",
      "\n",
      "================================================================================\n",
      "Model: BernoulliNB, Vectorizer: CountVectorizer, Accuracy: 0.8889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8942    0.8822    0.8882      5000\n",
      "           1     0.8838    0.8956    0.8896      5000\n",
      "\n",
      "    accuracy                         0.8889     10000\n",
      "   macro avg     0.8890    0.8889    0.8889     10000\n",
      "weighted avg     0.8890    0.8889    0.8889     10000\n",
      "\n",
      "================================================================================\n",
      "Model: BernoulliNB, Vectorizer: TfidfVectorizer, Accuracy: 0.8889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8942    0.8822    0.8882      5000\n",
      "           1     0.8838    0.8956    0.8896      5000\n",
      "\n",
      "    accuracy                         0.8889     10000\n",
      "   macro avg     0.8890    0.8889    0.8889     10000\n",
      "weighted avg     0.8890    0.8889    0.8889     10000\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train Models (MultinomialNB, BernoulliNB)\n",
    "# Theory Notes:\n",
    "# - MultinomialNB: Best for word counts or TF-IDF (works on frequencies).\n",
    "# - BernoulliNB: Best when features are binary (word present / absent).\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_and_eval(vec , clf, X_train, X_val, y_train, y_val):\n",
    "    # 1. Vectorize text\n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_val_vec = vec.transform(X_val)\n",
    "    \n",
    "    # 2. Train model\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # 3. Predict on validation data\n",
    "    preds = clf.predict(X_val_vec)\n",
    "    \n",
    "    # 4. Evaluate model\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    print(f\"Model: {clf.__class__.__name__}, Vectorizer: {vec.__class__.__name__}, Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_val, preds, digits=4))\n",
    "    print(\"=\" *80)\n",
    "    \n",
    "\n",
    "# Run experiments\n",
    "train_and_eval(bow_vectorizer, MultinomialNB(), X_train, X_val, y_train, y_val)\n",
    "train_and_eval(tfidf_vectorizer, MultinomialNB(), X_train, X_val, y_train, y_val)\n",
    "\n",
    "train_and_eval(bow_vectorizer, BernoulliNB(), X_train, X_val, y_train, y_val)\n",
    "train_and_eval(tfidf_vectorizer, BernoulliNB(), X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Notes:\n",
    "# - MultinomialNB + TF-IDF usually gives the best performance (≈85–90%).\n",
    "# - BernoulliNB is better when features are binary (word presence/absence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ba977db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.8849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Vectorizer     Classifier  Accuracy\n",
       "0  CountVectorizer  MultinomialNB    0.8849\n",
       "1  CountVectorizer    BernoulliNB    0.8889\n",
       "2  TfidfVectorizer  MultinomialNB    0.8919\n",
       "3  TfidfVectorizer    BernoulliNB    0.8889"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 6: Summary of Results\n",
    "#comparing all models side by side\n",
    "\n",
    "#Theory : We comapare which combination of features representation (BOW/TF-IDF) and classifier works best.\n",
    "\n",
    "results = []\n",
    "\n",
    "for vec in [bow_vectorizer, tfidf_vectorizer]:\n",
    "    for clf in [MultinomialNB(), BernoulliNB()]:\n",
    "        X_train_vec = vec.fit_transform(X_train)\n",
    "        X_val_vec = vec.transform(X_val)\n",
    "        \n",
    "        clf.fit(X_train_vec, y_train)\n",
    "        preds = clf.predict(X_val_vec)\n",
    "        acc = accuracy_score(y_val, preds)\n",
    "        \n",
    "        results.append({\n",
    "            \"Vectorizer\": vec.__class__.__name__,\n",
    "            \"Classifier\": clf.__class__.__name__,\n",
    "            \"Accuracy\": acc\n",
    "        })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2b5e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer + MultinomilaNB wins with accuracy = 89.19%\n",
    "\n",
    "#NOTEs : \n",
    "# 1) Naive Bayes : simple, fast, interpretable, assumes features independence, works well for text\n",
    "# 2) MultinomialNB : Best for counts/TF_IDF, assumes word frequencies as multinomial distribution.\n",
    "# 3) BernoulliNB : Binary features (present/absent), useful if only word presence matters.\n",
    "# 4) BOW vs TF-IDF : BOW=rawa frequency , TF-IDF : frequency weighted by importance, usually better for long text(like IMBD reviews)\n",
    "# 5) Evaluation : use accuracy and classificartion report(precision/recall/F1), important because accuracy alone may hide class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb5bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
